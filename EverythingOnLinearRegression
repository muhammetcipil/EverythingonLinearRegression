{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7876,"sourceType":"datasetVersion","datasetId":5227}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/muhammetipil/everything-on-linear-regression?scriptVersionId=182448456\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-09T14:25:33.81875Z","iopub.execute_input":"2024-06-09T14:25:33.819464Z","iopub.status.idle":"2024-06-09T14:25:33.829243Z","shell.execute_reply.started":"2024-06-09T14:25:33.819416Z","shell.execute_reply":"2024-06-09T14:25:33.82771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Herkese merhaba\n\nbu çalışma sayfamızda california house price veri setini kullanarak linear regresyion modeli ile house pricelerini tahmin etmeye çalıştık.\n\nÖncelikle veri setimizi projeye dahil ettik ve veriye hızlıca bi bakış attık. Daha sonra Eksik verileri gidermek için grupların medianından \nyararlandık. Daha sonra model için kategorik verileri label encoding ile modelde kullanımı için uygun hale getirdik.\n\nVeri görselleştirme ile değişkenlerin birbiri üzerinde olan etkilerini görsel olarak değerlendirdik. Ve Outlier detection adımı ile birlikte genel paterndan dışarıda olan verileri sınır verileri ile değiştirdik. Sınır verilerini belirlerken 1.5 quantile methodunu tercih ettik.\n\nDaha sonra Modelleme aşamasına geçtik. Bu aşamada oncelikle verimize standartization işlemi uyguladık,daha sonra özelliklerden korelasyon yardımı ile hangi verilerin elenmesi gerektiğini belirledik ve eleme yaptık. Daha sonra verimizi train ve test seti olarak ikiye ayırıp modelimizi fit ettik.\n\nModel Evaluation adımında modelimize ait R squared of linear regression,Adjacent R squared of linear regression,Mean  absolute precentage error of linear regression (MAPE),Mean  absolute error of linear regression (MAE),Mean  Squared Error of linear regression (MSE),Root Mean  Squared Error of linear regression (RMSE) verilerini değerlendirdik. \n\nVe ek olarak eğer verimiz modele uygun olsaydı varsayımları nasıl değerlendirirmemiz gerektiği ile ilgili ek kaynak bırkatım.\n\nElimizdeki veri linear modele çok uygun olmadığı için modelimiz çok verimli çalışmadı. Fakat buradaki amacım bir linear regression modelini ham veriden model oluşturmaya kadar gereken prosesi adım adım göstermekti. Bende sizlerele beraber yeni öğreniyorum.\n\n---------\n\nHello everyone\n\nIn this notebook,I tried to predict house prices with a linear regression model using the California house Price dataset.\n\nFirst of all, we loaded our dataset in this notebook and took a quick review at the data. than I used the median of  each groups to filling missing data.\n\nThen, we applied label encoding to the categorical data for the model suitable for use in the model.\n\nWith data visualization step I interested with effects of each variables on each other. Than I used 1.\nAnd with the Outlier detection step, we replaced the outlier with bound for each variable. I preferred to use 1.5 quantile method when determining the boundaries.\n\nThen we moved on to the Modeling stage. At this stage, we first applied the standardization for each column. Then we moved on Feature selection step. it was the last stage before one.In this stage I took into considaration correlation matris. \nThen, we divided our data into two as train and test set and fit our model.\n\nIn the Model Evaluation step we calculated;\n* R squared of linear regression, \n* Adjacent R squared of linear regression,\n* Mean absolute precentage error of linear regression (MAPE), \n* Mean absolute error of linear regression (MAE), \n* Mean Squared error of linear regression (MSE),\n* Root Mean squared Error of linear regression (RMSE).\n\nAnd in addition, I left additional resources on how we should evaluate the assumptions if our data were suitable for the model.\n\nSince the data we had was not very suitable for the linear model, our model did not work very efficiently. But my purpose here was to show step by step the process required to create a linear regression model from raw data to model.\n\nPlease keep in mind that I am still learning, so if you spot any incorrect explanations or anything, please let me know. Enjoy your journey, as well.","metadata":{}},{"cell_type":"markdown","source":"# Content\n* [Libraries](#chapter1)\n* [Loading Dataset And first review](#chapter2)\n* [Handling Missing Values](#chapter3)\n    * [Filling NAN values with Median of each speacial class](#chapter3.1)\n* [Label Encoding](#chapter4)\n* [Data Visualization](#chapter5)\n    * [Univariate histograms](#chapter5.1)\n    * [Bivariate correlation matrix](#chapter5.2)\n* [Outlier Decetion](#chapter6)\n    * [Defining the required function for outlier detection](#chapter6.1)\n    * [Visualisation Outlier by Box-Plot](#chapter6.2)\n    * [ Handling Outliers](#chapter6.3)\n* [Checking Duplicate's](#chapter7)\n* [Modelling](#chapter8)\n    * [Normalization](#chapter8.1)\n    * [Feature Selection](#chapter8.2)\n    * [Data Spliting ](#chapter8.3)\n    * [Fitting Model ](#chapter8.4)\n* [Model Evaluations](#chapter9)\n* [Plus +](#chapter9)","metadata":{}},{"cell_type":"markdown","source":"## Libraries <a class=\"anchor\"  id=\"chapter1\"></a>","metadata":{}},{"cell_type":"code","source":"# adding nescessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# -------------------------------------------\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score ,mean_absolute_percentage_error\n\n\n# -------------------------------------------\nfrom sklearn import linear_model\n\n# -------------------------------------------\nimport scipy\nfrom scipy.stats import shapiro\nfrom scipy import stats\n\n# -------------------------------------------\nfrom sklearn.model_selection import train_test_split\n\n# -------------------------------------------\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# -------------------------------------------\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:33.831916Z","iopub.execute_input":"2024-06-09T14:25:33.832349Z","iopub.status.idle":"2024-06-09T14:25:33.841752Z","shell.execute_reply.started":"2024-06-09T14:25:33.832306Z","shell.execute_reply":"2024-06-09T14:25:33.8404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Dataset And first review <a class=\"anchor\"  id=\"chapter2\"></a>","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/california-housing-prices/housing.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:33.843314Z","iopub.execute_input":"2024-06-09T14:25:33.84373Z","iopub.status.idle":"2024-06-09T14:25:33.918729Z","shell.execute_reply.started":"2024-06-09T14:25:33.843697Z","shell.execute_reply":"2024-06-09T14:25:33.917304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:33.92044Z","iopub.execute_input":"2024-06-09T14:25:33.920868Z","iopub.status.idle":"2024-06-09T14:25:33.976179Z","shell.execute_reply.started":"2024-06-09T14:25:33.920836Z","shell.execute_reply":"2024-06-09T14:25:33.974677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:33.979541Z","iopub.execute_input":"2024-06-09T14:25:33.979918Z","iopub.status.idle":"2024-06-09T14:25:33.999447Z","shell.execute_reply.started":"2024-06-09T14:25:33.979887Z","shell.execute_reply":"2024-06-09T14:25:33.998304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#looking for NaN values\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:34.001243Z","iopub.execute_input":"2024-06-09T14:25:34.001654Z","iopub.status.idle":"2024-06-09T14:25:34.017882Z","shell.execute_reply.started":"2024-06-09T14:25:34.00162Z","shell.execute_reply":"2024-06-09T14:25:34.016561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ## Handling Missing Values <a class=\"anchor\"  id=\"chapter3\"></a>","metadata":{}},{"cell_type":"code","source":"#checking null values\ndf[df[\"total_bedrooms\"].isna()].head(30)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:34.019369Z","iopub.execute_input":"2024-06-09T14:25:34.019822Z","iopub.status.idle":"2024-06-09T14:25:34.066733Z","shell.execute_reply.started":"2024-06-09T14:25:34.019785Z","shell.execute_reply":"2024-06-09T14:25:34.065246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filling NAN values with Median of each speacial class <a class=\"anchor\"  id=\"chapter3.1\"></a>","metadata":{}},{"cell_type":"markdown","source":"So for summerizing what ı've done with that.First, we divided the data into groups based on ocean proximity. And we separated the groups into 15 equal classes based on median income (we also determined the number of classes using the technique below formula). And we filled in the missing data with the **Median** of the class in which it occurred.","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/v2/resize:fit:490/format:webp/1*rJLA-SSiDQmuj0z5gJjq4Q.png)","metadata":{}},{"cell_type":"code","source":"#first we'll calculate bins with above's first code\nbin=round(1+3.322*np.log10(len(df[\"median_income\"])))\n#pd.cut(df['median_income'], bin) #its just giving 14 bins (as we define with bin)\nbin","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:34.068459Z","iopub.execute_input":"2024-06-09T14:25:34.068952Z","iopub.status.idle":"2024-06-09T14:25:34.079642Z","shell.execute_reply.started":"2024-06-09T14:25:34.068911Z","shell.execute_reply":"2024-06-09T14:25:34.077812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we grouped our data by ocean proximity than grouped in 15 group \ndf.groupby(['ocean_proximity',pd.cut(df['median_income'], bin)])[\"total_bedrooms\"].median().head(40)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:34.081496Z","iopub.execute_input":"2024-06-09T14:25:34.082003Z","iopub.status.idle":"2024-06-09T14:25:34.114352Z","shell.execute_reply.started":"2024-06-09T14:25:34.081964Z","shell.execute_reply":"2024-06-09T14:25:34.112907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first group by ocean proximity then groupby bins which I gave with cut function\ndf[\"total_bedrooms\"]=df.groupby(['ocean_proximity',pd.cut(df['median_income'], bin)])[\"total_bedrooms\"].transform(lambda x: x.fillna(x.median()))","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:34.11607Z","iopub.execute_input":"2024-06-09T14:25:34.116404Z","iopub.status.idle":"2024-06-09T14:25:34.166375Z","shell.execute_reply.started":"2024-06-09T14:25:34.116376Z","shell.execute_reply":"2024-06-09T14:25:34.165304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking how formula filled nan values for INLAND \ndf.iloc[1097]\n#as conclusion model works perfect ","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:34.16779Z","iopub.execute_input":"2024-06-09T14:25:34.168097Z","iopub.status.idle":"2024-06-09T14:25:34.176787Z","shell.execute_reply.started":"2024-06-09T14:25:34.168071Z","shell.execute_reply":"2024-06-09T14:25:34.175448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label Encoding <a class=\"anchor\"  id=\"chapter4\"></a>","metadata":{}},{"cell_type":"code","source":"# Import label encoder \nfrom sklearn import preprocessing \n  \n# label_encoder object knows  \n# how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \n# Encode labels in column 'species'. \ndf['ocean_proximity']= label_encoder.fit_transform(df['ocean_proximity']) \n  \nprint(df['ocean_proximity'].unique()) \ndf","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:34.178304Z","iopub.execute_input":"2024-06-09T14:25:34.178693Z","iopub.status.idle":"2024-06-09T14:25:34.219815Z","shell.execute_reply.started":"2024-06-09T14:25:34.178662Z","shell.execute_reply":"2024-06-09T14:25:34.218464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualization <a class=\"anchor\"  id=\"chapter5\"></a>\n","metadata":{}},{"cell_type":"markdown","source":"### Univariate histograms <a class=\"anchor\"  id=\"chapter5.1\"></a>","metadata":{}},{"cell_type":"code","source":"df.hist(bins=60, figsize=(15,9));plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:34.221199Z","iopub.execute_input":"2024-06-09T14:25:34.221549Z","iopub.status.idle":"2024-06-09T14:25:37.061158Z","shell.execute_reply.started":"2024-06-09T14:25:34.221498Z","shell.execute_reply":"2024-06-09T14:25:37.059848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bivariate correlation matrix <a class=\"anchor\"  id=\"chapter5.2\"></a>","metadata":{}},{"cell_type":"code","source":"corr=df.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool_))\n\nsns.heatmap(corr,lw=2,annot=True,square=False,mask=mask)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:37.068173Z","iopub.execute_input":"2024-06-09T14:25:37.068589Z","iopub.status.idle":"2024-06-09T14:25:38.298235Z","shell.execute_reply.started":"2024-06-09T14:25:37.068555Z","shell.execute_reply":"2024-06-09T14:25:38.296936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outlier Detection <a class=\"anchor\"  id=\"chapter6\"></a>","metadata":{}},{"cell_type":"markdown","source":"### Defining the required function for outlier detection <a class=\"anchor\"  id=\"chapter6.1\"></a>","metadata":{}},{"cell_type":"code","source":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\n\ndef outlier_detect(df, col):\n    q1_col = Q1[col]\n    iqr_col = IQR[col]\n    q3_col = Q3[col]\n    return df[((df[col] < (q1_col - 1.5 * iqr_col)) |(df[col] > (q3_col + 1.5 * iqr_col)))]\n\n# -------------------------------------------\ndef lower_outlier(df, col):\n    q1_col = Q1[col]\n    iqr_col = IQR[col]\n    q3_col = Q3[col]\n    lower = df[(df[col] < (q1_col - 1.5 * iqr_col))]\n    return lower\n\n# -------------------------------------------\ndef upper_outlier(df, col):\n    q1_col = Q1[col]\n    iqr_col = IQR[col]\n    q3_col = Q3[col]\n    upper = df[(df[col] > (q3_col + 1.5 * iqr_col))]\n    return upper\n\n# -------------------------------------------\ndef replace_upper(df, col):\n    q1_col = Q1[col]\n    iqr_col = IQR[col]\n    q3_col = Q3[col]\n    tmp = 9999999\n    upper = q3_col + 1.5 * iqr_col\n    df[col] = df[col].where(lambda x: (x < (upper)), tmp)\n    df[col] = df[col].replace(tmp, upper)\n    print('outlire replace with upper bound - {}' .format(col)) \n    \n# -------------------------------------------\ndef replace_lower(df, col):\n    q1_col = Q1[col]\n    iqr_col = IQR[col]\n    q3_col = Q3[col]\n    tmp = 1111111\n    lower = q1_col - 1.5 * iqr_col\n    df[col] = df[col].where(lambda x: (x > (lower)), tmp)\n    df[col] = df[col].replace(tmp, lower)\n    print('outlire replace with lower bound - {}' .format(col)) \n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:38.300193Z","iopub.execute_input":"2024-06-09T14:25:38.301845Z","iopub.status.idle":"2024-06-09T14:25:38.331856Z","shell.execute_reply.started":"2024-06-09T14:25:38.301795Z","shell.execute_reply":"2024-06-09T14:25:38.330481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualisation Outlier by Box-Plot <a class=\"anchor\"  id=\"chapter6.2\"></a>","metadata":{}},{"cell_type":"code","source":"df.plot(kind=\"box\",subplots=True,layout=(7,2),figsize=(15,20));","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:38.333878Z","iopub.execute_input":"2024-06-09T14:25:38.334698Z","iopub.status.idle":"2024-06-09T14:25:40.100073Z","shell.execute_reply.started":"2024-06-09T14:25:38.334653Z","shell.execute_reply":"2024-06-09T14:25:40.098586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we needs to exclude ocean proximity because it's a categorical data\nnum_column_names=df.drop(\"ocean_proximity\",axis=1).columns","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:40.102083Z","iopub.execute_input":"2024-06-09T14:25:40.102614Z","iopub.status.idle":"2024-06-09T14:25:40.111451Z","shell.execute_reply.started":"2024-06-09T14:25:40.10257Z","shell.execute_reply":"2024-06-09T14:25:40.109973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(num_column_names)):\n    print(\"Outlier numbers => {}: {}\".format(num_column_names[i],(outlier_detect(df[num_column_names],num_column_names[i]).shape[0])))","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:40.113705Z","iopub.execute_input":"2024-06-09T14:25:40.114563Z","iopub.status.idle":"2024-06-09T14:25:40.14367Z","shell.execute_reply.started":"2024-06-09T14:25:40.1145Z","shell.execute_reply":"2024-06-09T14:25:40.142335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Outliers <a class=\"anchor\"  id=\"chapter6.3\"></a>","metadata":{}},{"cell_type":"code","source":"for i in range(len(num_column_names)):\n    replace_upper(df, num_column_names[i]) \n    \nprint(\"\\n*******************************************\\n\")\nfor i in range(len(num_column_names)):\n    replace_lower(df, num_column_names[i])","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:40.145109Z","iopub.execute_input":"2024-06-09T14:25:40.145472Z","iopub.status.idle":"2024-06-09T14:25:40.180898Z","shell.execute_reply.started":"2024-06-09T14:25:40.145441Z","shell.execute_reply":"2024-06-09T14:25:40.179398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(num_column_names)):\n    print(\"Outlier numbers => {}: {}\".format(num_column_names[i],(outlier_detect(df[num_column_names],num_column_names[i]).shape[0])))","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:40.1827Z","iopub.execute_input":"2024-06-09T14:25:40.183185Z","iopub.status.idle":"2024-06-09T14:25:40.216895Z","shell.execute_reply.started":"2024-06-09T14:25:40.183133Z","shell.execute_reply":"2024-06-09T14:25:40.215378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:40.219007Z","iopub.execute_input":"2024-06-09T14:25:40.219485Z","iopub.status.idle":"2024-06-09T14:25:40.250383Z","shell.execute_reply.started":"2024-06-09T14:25:40.219446Z","shell.execute_reply":"2024-06-09T14:25:40.249194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.plot(kind=\"box\",subplots=True,layout=(7,2),figsize=(15,20));","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:40.252333Z","iopub.execute_input":"2024-06-09T14:25:40.252841Z","iopub.status.idle":"2024-06-09T14:25:41.902187Z","shell.execute_reply.started":"2024-06-09T14:25:40.252784Z","shell.execute_reply":"2024-06-09T14:25:41.90087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking Duplicate's <a class=\"anchor\"  id=\"chapter7\"></a>","metadata":{}},{"cell_type":"code","source":"df[df.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:41.903682Z","iopub.execute_input":"2024-06-09T14:25:41.904067Z","iopub.status.idle":"2024-06-09T14:25:41.929624Z","shell.execute_reply.started":"2024-06-09T14:25:41.904036Z","shell.execute_reply":"2024-06-09T14:25:41.928128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:41.931245Z","iopub.execute_input":"2024-06-09T14:25:41.931711Z","iopub.status.idle":"2024-06-09T14:25:41.941236Z","shell.execute_reply.started":"2024-06-09T14:25:41.931673Z","shell.execute_reply":"2024-06-09T14:25:41.939826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling <a class=\"anchor\"  id=\"chapter8\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### Normalization <a class=\"anchor\"  id=\"chapter8.1\"></a>","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"scale_features = StandardScaler()\ndf[num_column_names] = scale_features.fit_transform(df[num_column_names])\ndf\n\n#first ı used to minmax scaler then ı wanted to see with StandartScaler if is it distrubuting normally my data ?","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:41.943329Z","iopub.execute_input":"2024-06-09T14:25:41.943876Z","iopub.status.idle":"2024-06-09T14:25:41.981593Z","shell.execute_reply.started":"2024-06-09T14:25:41.943833Z","shell.execute_reply":"2024-06-09T14:25:41.980135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(num_column_names)):\n    stat,p = shapiro(df[num_column_names[i]])\n    print(num_column_names[i])\n    print('P_value=%.3f' % (p))\n    print(\"*******************************\")\n# ı check the transformed data if it's normally distributed,but the result not normally distributed","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:41.983519Z","iopub.execute_input":"2024-06-09T14:25:41.984026Z","iopub.status.idle":"2024-06-09T14:25:42.013623Z","shell.execute_reply.started":"2024-06-09T14:25:41.983985Z","shell.execute_reply":"2024-06-09T14:25:42.012186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Selection <a class=\"anchor\"  id=\"chapter8.2\"></a>\nHere we gonna drop **unimportant**(low correlation) columns\n    \n    * Method;Correlation Coefficient","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df.corr(),annot=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:42.015259Z","iopub.execute_input":"2024-06-09T14:25:42.015701Z","iopub.status.idle":"2024-06-09T14:25:42.803183Z","shell.execute_reply.started":"2024-06-09T14:25:42.015667Z","shell.execute_reply":"2024-06-09T14:25:42.801671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Assuming a threshold of 90% for correlation coefficient, \n\n    * We must consider latitude and longitude. However, these variables may not be useful for prediction.\n\n    * And I decided to continue 'housing_median_age','total_rooms', 'total_bedrooms', 'population', 'households','median_income','median_house_value' columns for prediction.","metadata":{}},{"cell_type":"code","source":"df=df[['housing_median_age',\n       'total_rooms', 'total_bedrooms', 'population', 'households',\n       'median_income','median_house_value']]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:42.804787Z","iopub.execute_input":"2024-06-09T14:25:42.805159Z","iopub.status.idle":"2024-06-09T14:25:42.814187Z","shell.execute_reply.started":"2024-06-09T14:25:42.805121Z","shell.execute_reply":"2024-06-09T14:25:42.812605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Spliting <a class=\"anchor\"  id=\"chapter8.3\"></a>","metadata":{}},{"cell_type":"code","source":"X = df.drop(['median_house_value'], axis=1)\n\ny= df['median_house_value']","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:42.8443Z","iopub.execute_input":"2024-06-09T14:25:42.844748Z","iopub.status.idle":"2024-06-09T14:25:42.854745Z","shell.execute_reply.started":"2024-06-09T14:25:42.844714Z","shell.execute_reply":"2024-06-09T14:25:42.853156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train , X_test , y_train , y_test = train_test_split(X,y , test_size= 0.3 , random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:42.856663Z","iopub.execute_input":"2024-06-09T14:25:42.857902Z","iopub.status.idle":"2024-06-09T14:25:42.872155Z","shell.execute_reply.started":"2024-06-09T14:25:42.85786Z","shell.execute_reply":"2024-06-09T14:25:42.870519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting Model <a class=\"anchor\"  id=\"chapter8.4\"></a>","metadata":{}},{"cell_type":"markdown","source":"##### Linear Regression\n\nNormally we shouldnt use linear regression model because its not meeting with assumption","metadata":{}},{"cell_type":"code","source":"reg=linear_model.LinearRegression()\nreg.fit(X_train,y_train)\nreg.score(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:25:42.874009Z","iopub.execute_input":"2024-06-09T14:25:42.874416Z","iopub.status.idle":"2024-06-09T14:25:42.902569Z","shell.execute_reply.started":"2024-06-09T14:25:42.874381Z","shell.execute_reply":"2024-06-09T14:25:42.90087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict=reg.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:45:50.607268Z","iopub.execute_input":"2024-06-09T14:45:50.608284Z","iopub.status.idle":"2024-06-09T14:45:50.619705Z","shell.execute_reply.started":"2024-06-09T14:45:50.608244Z","shell.execute_reply":"2024-06-09T14:45:50.617893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluations <a class=\"anchor\"  id=\"chapter9\"></a>","metadata":{}},{"cell_type":"code","source":"# For Linear Regression\nreg_score = r2_score(y_test , y_predict)\nprint('R squared of linear regression :',reg_score)\n\np = len(X_train[\"housing_median_age\"])\nn = len(y_train)\nadj_R2 = 1-(1-reg_score)*(n-1)/(n-p-1)\nprint('Adjacent R squared of linear regression : ',adj_R2)\n\nmape = mean_absolute_percentage_error(y_test , y_predict)\nprint(\"Mean  absolute precentage error of linear regression (MAPE) : \",mape*100,'%')\n\nreg_mae = mean_absolute_error(y_test , y_predict)\nprint(\"Mean  absolute error of linear regression (MAE): \",reg_mae)\n\nmse = mean_squared_error(y_test , y_predict)\nprint(\"Mean  Squared error of linear regression (MSE): \",mse)\n\nrmse = np.square((mean_squared_error(y_test , y_predict)))\nprint(\"Mean  Squared error of linear regression (RMSE): \",rmse)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T14:46:13.965617Z","iopub.execute_input":"2024-06-09T14:46:13.966039Z","iopub.status.idle":"2024-06-09T14:46:13.982021Z","shell.execute_reply.started":"2024-06-09T14:46:13.96601Z","shell.execute_reply":"2024-06-09T14:46:13.980415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing these metrics:\n\nMAE is the easiest to understand, because it's the average error.\nMSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\nRMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\nAll of these are loss functions, because we want to minimize them.","metadata":{}},{"cell_type":"markdown","source":"# Plus +  <a class=\"anchor\"  id=\"chapter10\"></a>","metadata":{}},{"cell_type":"markdown","source":"### Assumptions of Linear Regression\n\n1. Linearity of residuals\n1. İndepence of residuals\n1. Normal distribution of residuals\n1. Equal distribution of residuals\n\n[You can Check this article](https://www.analyticsvidhya.com/blog/2021/10/everything-you-need-to-know-about-linear-regression/#:~:text=The%20goal%20of%20the%20linear,actual%20values%20should%20be%20minimum.)","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\nNormally, it is necessary to test the assumptions, but our evaluation metrics show us that the model is not very accurate. Therefore, testing these only helps us support that our data is not suitable for the linear regression model. That's why we don't test them.\n \n But for those who are wondering how it is tested, I leave a detailed [notebook](https://www.kaggle.com/code/shrutimechlearn/step-by-step-assumptions-linear-regression).","metadata":{}}]}